{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d44420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"transactions_train.csv\", dtype={\"article_id\": str})\n",
    "df_article = pd.read_csv(\"articles.csv\", dtype={\"article_id\": str})\n",
    "\n",
    "df = df.tail(2000000).reset_index(drop=True)\n",
    "df[\"t_dat\"] = pd.to_datetime(df[\"t_dat\"])\n",
    "df[\"week\"] = (df[\"t_dat\"].max() - df[\"t_dat\"]).dt.days // 7\n",
    "\n",
    "article_count = df[\"article_id\"].value_counts()\n",
    "df = df[df[\"article_id\"].map(article_count) >= 50]\n",
    "top_articles_df = df[\"article_id\"].value_counts().head(100).reset_index()\n",
    "top_articles_df.columns = [\"article_id\", \"counts\"]\n",
    "df_article = df_article.merge(top_articles_df, on=\"article_id\", how=\"right\")\n",
    "df_article = df_article.set_index(\"article_id\")\n",
    "\n",
    "top_articles = top_articles_df[\"article_id\"]\n",
    "df_article = df_article.loc[top_articles]\n",
    "df_article[[\"prod_name\", \"product_code\", \"product_type_name\", \"counts\", \"perceived_colour_value_name\"]]\n",
    "df_article[\"product_type_name\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEK_HIST_MAX = 5\n",
    "df[\"article_id\"] = pd.factorize(df['article_id'])[0]\n",
    "article_ids = np.unique(df[\"article_id\"].values)\n",
    "\n",
    "def create_dataset(df, week):\n",
    "    hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + WEEK_HIST_MAX)]\n",
    "    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n",
    "    \n",
    "    target_df = df[df[\"week\"] == week]           \n",
    "    target_df = target_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n",
    "    target_df.rename(columns={\"article_id\": \"target\"}, inplace=True)\n",
    "    target_df[\"week\"] = week\n",
    "    target_df = target_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n",
    "    target_df = target_df[target_df[\"article_id\"].notna()].reset_index(drop=True)\n",
    "    target_df = target_df[target_df[\"target\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    return target_df\n",
    "\n",
    "test_weeks = [0]\n",
    "train_weeks = [1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3f158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = create_dataset(df, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d970fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12859984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, seq_len):\n",
    "        self.df = df\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        article_hist = torch.zeros(self.seq_len).long()\n",
    "        target = torch.zeros(len(article_ids)).float()\n",
    "\n",
    "        for t in row.target:\n",
    "            target[t] = 1.0\n",
    "                    \n",
    "        if len(row.article_id) >= self.seq_len:\n",
    "            article_hist = torch.LongTensor(row.article_id[-self.seq_len:])\n",
    "        else:\n",
    "            article_hist[-len(row.article_id):] = torch.LongTensor(row.article_id)\n",
    "                \n",
    "        return article_hist, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e93b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAttentiveRecMachine(nn.Module):\n",
    "    def __init__(self, article_dim):\n",
    "        super(NeuralAttentiveRecMachine, self).__init__()\n",
    "        \n",
    "        self.article_emb = nn.Embedding(len(article_ids), article_dim)\n",
    "        self.encoder = nn.GRU(article_dim, article_dim, batch_first=True)\n",
    "        self.attention_layer = nn.Linear(article_dim, article_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.context_layer = nn.Linear(2*article_dim, article_dim)\n",
    "        \n",
    "    def forward(self, article_hist):\n",
    "        tensor = self.article_emb(article_hist)\n",
    "        encoder_output, hn = self.encoder(tensor)\n",
    "        hn = hn.squeeze(0)\n",
    "        attention_logits = self.attention_layer(hn).unsqueeze(1) @ encoder_output.transpose(1, 2)\n",
    "        attention_weights = self.softmax(attention_logits)\n",
    "        context_vector = torch.concat([hn, (attention_weights @ encoder_output).squeeze(1)], -1)\n",
    "        context_vector = self.context_layer(context_vector)\n",
    "        article_embs = self.article_emb(torch.tensor(article_ids))\n",
    "        score = context_vector @ article_embs.transpose(0, 1)\n",
    "        \n",
    "        return score\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruforRec(nn.Module):\n",
    "    def __init__(self, article_dim):\n",
    "        super(GruforRec, self).__init__()\n",
    "        self.article_emb = nn.Embedding(len(article_ids), article_dim)\n",
    "        self.encoder = nn.GRU(article_dim, article_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, article_hist):\n",
    "        tensor = self.article_emb(article_hist)\n",
    "        _, hn = self.encoder(tensor)\n",
    "        hn = hn.squeeze(0)\n",
    "        article_embs = self.article_emb(torch.tensor(article_ids))\n",
    "        score = hn @ article_embs.transpose(0, 1)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recall(output, target):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    output = sigmoid(output)\n",
    "    recall = output > 0.4\n",
    "    recall = ((recall == target.bool()) & recall).float().sum(dim=1) / (target.sum(dim=1) + 1e-10)\n",
    "    recall = recall.sum(dim=0) / recall.shape[0]\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40588de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_precision(output, target):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    output = sigmoid(output)\n",
    "    precision = output > 0.4\n",
    "    precision = ((precision == target.bool()) & precision).float().sum(dim=1) / (precision.float().sum(dim=1) + 1e-10)\n",
    "    precision = precision.sum(dim=0) / precision.shape[0]\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545082bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 16\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30\n",
    "\n",
    "test_df = pd.concat([create_dataset(df, w) for w in test_weeks]).reset_index(drop=True)\n",
    "train_df = pd.concat([create_dataset(df, w) for w in train_weeks]).reset_index(drop=True)\n",
    "train_dataset = Dataset(train_df, SEQ_LEN)\n",
    "test_dataset = Dataset(test_df, SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model1 = NeuralAttentiveRecMachine(50)\n",
    "model2 = GruforRec(50)\n",
    "\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=LEARNING_RATE)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23832d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Current Epoch {i+1}\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Training..\")\n",
    "    \n",
    "    _loss = .0\n",
    "    train_recall = []\n",
    "    train_precision = []\n",
    "    model1.train()\n",
    "    for article_hist, target in tqdm(train_loader):\n",
    "        loss = criterion(model1(article_hist), target)\n",
    "        _loss += loss.item()\n",
    "        train_recall.append(eval_recall(model1(article_hist), target))\n",
    "        train_precision.append(eval_precision(model1(article_hist), target))\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "    \n",
    "    print(\"Evaluating..\")\n",
    "    _eval_loss = .0\n",
    "    test_recall = []\n",
    "    test_precision = []\n",
    "    model1.eval()\n",
    "    for article_hist, target in tqdm(test_loader):\n",
    "        eval_loss = criterion(model1(article_hist), target)\n",
    "        test_recall.append(eval_recall(model1(article_hist), target))\n",
    "        test_precision.append(eval_precision(model1(article_hist), target))\n",
    "\n",
    "        _eval_loss += eval_loss.item()\n",
    "    \n",
    "    train_loss = _loss/len(train_loader.dataset) * 100\n",
    "    eval_loss = _eval_loss/len(test_loader.dataset) * 100\n",
    "    train_recall = torch.mean(torch.tensor(train_recall))\n",
    "    test_recall = torch.mean(torch.tensor(test_recall))\n",
    "    train_precision = torch.mean(torch.tensor(train_precision))\n",
    "    test_precision = torch.mean(torch.tensor(test_precision))\n",
    "    print(f\"Epoch {i+1}\\nTrain Loss: {train_loss}\\nTest Loss: {eval_loss}\\nTrain Recall: {train_recall}\\nTest Recall: {test_recall}\\nTrain Precision: {train_precision}\\nTest Precision: {test_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee83305",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Current Epoch {i+1}\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Training..\")\n",
    "    \n",
    "    _loss = .0\n",
    "    train_recall = []\n",
    "    train_precision = []\n",
    "    model2.train()\n",
    "    for article_hist, target in tqdm(train_loader):\n",
    "        loss = criterion(model2(article_hist), target)\n",
    "        _loss += loss.item()\n",
    "        train_recall.append(eval_recall(model2(article_hist), target))\n",
    "        train_precision.append(eval_precision(model2(article_hist), target))\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "    \n",
    "    print(\"Evaluating..\")\n",
    "    _eval_loss = .0\n",
    "    test_recall = []\n",
    "    test_precision = []\n",
    "    model2.eval()\n",
    "    for article_hist, target in tqdm(test_loader):\n",
    "        eval_loss = criterion(model2(article_hist), target)\n",
    "        test_recall.append(eval_recall(model2(article_hist), target))\n",
    "        test_precision.append(eval_precision(model2(article_hist), target))\n",
    "\n",
    "        _eval_loss += eval_loss.item()\n",
    "    \n",
    "    train_loss = _loss/len(train_loader.dataset) * 100\n",
    "    eval_loss = _eval_loss/len(test_loader.dataset) * 100\n",
    "    train_recall = torch.mean(torch.tensor(train_recall))\n",
    "    test_recall = torch.mean(torch.tensor(test_recall))\n",
    "    train_precision = torch.mean(torch.tensor(train_precision))\n",
    "    test_precision = torch.mean(torch.tensor(test_precision))\n",
    "    print(f\"Epoch {i+1}\\nTrain Loss: {train_loss}\\nTest Loss: {eval_loss}\\nTrain Recall: {train_recall}\\nTest Recall: {test_recall}\\nTrain Precision: {train_precision}\\nTest Precision: {test_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f354ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
