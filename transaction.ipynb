{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "d7d44420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "abdd4e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_type_name\n",
       "Trousers            20\n",
       "T-shirt             18\n",
       "Sweater             10\n",
       "Leggings/Tights      6\n",
       "Cardigan             6\n",
       "Vest top             6\n",
       "Hoodie               5\n",
       "Blouse               4\n",
       "Underwear bottom     4\n",
       "Blazer               4\n",
       "Shirt                4\n",
       "Socks                3\n",
       "Unknown              2\n",
       "Top                  2\n",
       "Bra                  2\n",
       "Bikini top           2\n",
       "Dress                1\n",
       "Sarong               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"transactions_train.csv\", dtype={\"article_id\": str})\n",
    "df_article = pd.read_csv(\"articles.csv\", dtype={\"article_id\": str})\n",
    "\n",
    "df = df.tail(2000000).reset_index(drop=True)\n",
    "df[\"t_dat\"] = pd.to_datetime(df[\"t_dat\"])\n",
    "df[\"week\"] = (df[\"t_dat\"].max() - df[\"t_dat\"]).dt.days // 7\n",
    "\n",
    "article_count = df[\"article_id\"].value_counts()\n",
    "df = df[df[\"article_id\"].map(article_count) >= 50]\n",
    "top_articles_df = df[\"article_id\"].value_counts().head(100).reset_index()\n",
    "top_articles_df.columns = [\"article_id\", \"counts\"]\n",
    "df_article = df_article.merge(top_articles_df, on=\"article_id\", how=\"right\")\n",
    "df_article = df_article.set_index(\"article_id\")\n",
    "\n",
    "top_articles = top_articles_df[\"article_id\"]\n",
    "df_article = df_article.loc[top_articles]\n",
    "df_article[[\"prod_name\", \"product_code\", \"product_type_name\", \"counts\", \"perceived_colour_value_name\"]]\n",
    "df_article[\"product_type_name\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "4ac9ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEK_HIST_MAX = 5\n",
    "df[\"article_id\"] = pd.factorize(df['article_id'])[0]\n",
    "article_ids = np.unique(df[\"article_id\"].values)\n",
    "\n",
    "def create_dataset(df, week):\n",
    "    hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + WEEK_HIST_MAX)]\n",
    "    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n",
    "    \n",
    "    target_df = df[df[\"week\"] == week]           \n",
    "    target_df = target_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n",
    "    target_df.rename(columns={\"article_id\": \"target\"}, inplace=True)\n",
    "    target_df[\"week\"] = week\n",
    "    target_df = target_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n",
    "    target_df = target_df[target_df[\"article_id\"].notna()].reset_index(drop=True)\n",
    "    target_df = target_df[target_df[\"target\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    return target_df\n",
    "\n",
    "test_weeks = [0]\n",
    "train_weeks = [1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "876bab94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>target</th>\n",
       "      <th>week</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001d44dbe7f6c4b35200abdb052c77a87596fe1bdcc37...</td>\n",
       "      <td>[6535, 6724, 1887, 7046, 2593]</td>\n",
       "      <td>2</td>\n",
       "      <td>[2057, 215, 2592, 1400, 2593, 200, 1510, 108, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f8cef6b9702d54abf66fd89eb21014bf98567065a9...</td>\n",
       "      <td>[3633]</td>\n",
       "      <td>2</td>\n",
       "      <td>[2505, 637, 1108]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003e867a930d0d6842f923d6ba7c9b77aba33fe2a0fbf...</td>\n",
       "      <td>[7076, 6605, 6734]</td>\n",
       "      <td>2</td>\n",
       "      <td>[2082, 248, 248, 1581, 6217, 752]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006d3ff0caf0cb4d4e0615ee5cb7d268622364d483335...</td>\n",
       "      <td>[1576, 4795]</td>\n",
       "      <td>2</td>\n",
       "      <td>[553, 5605, 1790, 1739, 3216, 1576, 5355, 4436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00075ef36696a7b4ed8c83e22a4bf7ea7c90ee110991ec...</td>\n",
       "      <td>[2788, 7018]</td>\n",
       "      <td>2</td>\n",
       "      <td>[939, 2707, 1674, 1421, 3681, 1131, 3119, 1752...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32441</th>\n",
       "      <td>fff5506ea8a342e778e4f2fbc2c9575e20b71cf24b75e6...</td>\n",
       "      <td>[3794]</td>\n",
       "      <td>2</td>\n",
       "      <td>[3527]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32442</th>\n",
       "      <td>fff5a8e958488dc8a6f24f65bd1b40fb733068eb2cb54f...</td>\n",
       "      <td>[1270]</td>\n",
       "      <td>2</td>\n",
       "      <td>[2574, 3113]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32443</th>\n",
       "      <td>fff60c2d6ade407465a875a5640a1e6e2b5ed5f6ec21f5...</td>\n",
       "      <td>[16, 15, 6313]</td>\n",
       "      <td>2</td>\n",
       "      <td>[244, 245, 248]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32444</th>\n",
       "      <td>fff7e7674509592818bf453391af43a85eaaac9a52d858...</td>\n",
       "      <td>[4607]</td>\n",
       "      <td>2</td>\n",
       "      <td>[2078, 116, 3836, 2293, 3313, 5808, 4607]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32445</th>\n",
       "      <td>ffffbbf78b6eaac697a8a5dfbfd2bfa8113ee5b403e474...</td>\n",
       "      <td>[4880, 293, 538, 462, 4751, 1107]</td>\n",
       "      <td>2</td>\n",
       "      <td>[339, 1971, 1846, 1082, 6297]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32446 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             customer_id  \\\n",
       "0      0001d44dbe7f6c4b35200abdb052c77a87596fe1bdcc37...   \n",
       "1      0001f8cef6b9702d54abf66fd89eb21014bf98567065a9...   \n",
       "2      0003e867a930d0d6842f923d6ba7c9b77aba33fe2a0fbf...   \n",
       "3      0006d3ff0caf0cb4d4e0615ee5cb7d268622364d483335...   \n",
       "4      00075ef36696a7b4ed8c83e22a4bf7ea7c90ee110991ec...   \n",
       "...                                                  ...   \n",
       "32441  fff5506ea8a342e778e4f2fbc2c9575e20b71cf24b75e6...   \n",
       "32442  fff5a8e958488dc8a6f24f65bd1b40fb733068eb2cb54f...   \n",
       "32443  fff60c2d6ade407465a875a5640a1e6e2b5ed5f6ec21f5...   \n",
       "32444  fff7e7674509592818bf453391af43a85eaaac9a52d858...   \n",
       "32445  ffffbbf78b6eaac697a8a5dfbfd2bfa8113ee5b403e474...   \n",
       "\n",
       "                                  target  week  \\\n",
       "0         [6535, 6724, 1887, 7046, 2593]     2   \n",
       "1                                 [3633]     2   \n",
       "2                     [7076, 6605, 6734]     2   \n",
       "3                           [1576, 4795]     2   \n",
       "4                           [2788, 7018]     2   \n",
       "...                                  ...   ...   \n",
       "32441                             [3794]     2   \n",
       "32442                             [1270]     2   \n",
       "32443                     [16, 15, 6313]     2   \n",
       "32444                             [4607]     2   \n",
       "32445  [4880, 293, 538, 462, 4751, 1107]     2   \n",
       "\n",
       "                                              article_id  \n",
       "0      [2057, 215, 2592, 1400, 2593, 200, 1510, 108, ...  \n",
       "1                                      [2505, 637, 1108]  \n",
       "2                      [2082, 248, 248, 1581, 6217, 752]  \n",
       "3      [553, 5605, 1790, 1739, 3216, 1576, 5355, 4436...  \n",
       "4      [939, 2707, 1674, 1421, 3681, 1131, 3119, 1752...  \n",
       "...                                                  ...  \n",
       "32441                                             [3527]  \n",
       "32442                                       [2574, 3113]  \n",
       "32443                                    [244, 245, 248]  \n",
       "32444          [2078, 116, 3836, 2293, 3313, 5808, 4607]  \n",
       "32445                      [339, 1971, 1846, 1082, 6297]  \n",
       "\n",
       "[32446 rows x 4 columns]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = create_dataset(df, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "76d970fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "12859984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, seq_len):\n",
    "        self.df = df\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        article_hist = torch.zeros(self.seq_len).long()\n",
    "        target = torch.zeros(len(article_ids)).float()\n",
    "\n",
    "        for t in row.target:\n",
    "            target[t] = 1.0\n",
    "                    \n",
    "        if len(row.article_id) >= self.seq_len:\n",
    "            article_hist = torch.LongTensor(row.article_id[-self.seq_len:])\n",
    "        else:\n",
    "            article_hist[-len(row.article_id):] = torch.LongTensor(row.article_id)\n",
    "                \n",
    "        return article_hist, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "e4a6902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAttentiveRecMachine(nn.Module):\n",
    "    def __init__(self, article_dim):\n",
    "        super(NeuralAttentiveRecMachine, self).__init__()\n",
    "        \n",
    "        self.article_emb = nn.Embedding(len(article_ids), article_dim)\n",
    "        self.encoder = nn.GRU(article_dim, article_dim, batch_first=True)\n",
    "        self.attention_layer = nn.Linear(article_dim, article_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.context_layer = nn.Linear(2*article_dim, article_dim)\n",
    "        \n",
    "    def forward(self, article_hist):\n",
    "        tensor = self.article_emb(article_hist)\n",
    "        encoder_output, hn = self.encoder(tensor)\n",
    "        hn = hn.squeeze(0)\n",
    "        attention_logits = self.attention_layer(hn).unsqueeze(1) @ encoder_output.transpose(1, 2)\n",
    "        attention_weights = self.softmax(attention_logits)\n",
    "        context_vector = torch.concat([hn, (attention_weights @ encoder_output).squeeze(1)], -1)\n",
    "        context_vector = self.context_layer(context_vector)\n",
    "        article_embs = self.article_emb(torch.tensor(article_ids))\n",
    "        score = context_vector @ article_embs.transpose(0, 1)\n",
    "        \n",
    "        return score\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "1dc4fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruforRec(nn.Module):\n",
    "    def __init__(self, article_dim):\n",
    "        super(GruforRec, self).__init__()\n",
    "        self.article_emb = nn.Embedding(len(article_ids), article_dim)\n",
    "        self.encoder = nn.GRU(article_dim, article_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, article_hist):\n",
    "        tensor = self.article_emb(article_hist)\n",
    "        _, hn = self.encoder(tensor)\n",
    "        hn = hn.squeeze(0)\n",
    "        article_embs = self.article_emb(torch.tensor(article_ids))\n",
    "        score = hn @ article_embs.transpose(0, 1)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "066dbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recall(output, target):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    output = sigmoid(output)\n",
    "    recall = output > 0.4\n",
    "    recall = ((recall == target.bool()) & recall).float().sum(dim=1) / (target.sum(dim=1) + 1e-10)\n",
    "    recall = recall.sum(dim=0) / recall.shape[0]\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "a359b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_precision(output, target):\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    output = sigmoid(output)\n",
    "    precision = output > 0.4\n",
    "    precision = ((precision == target.bool()) & precision).float().sum(dim=1) / (precision.float().sum(dim=1) + 1e-10)\n",
    "    precision = precision.sum(dim=0) / precision.shape[0]\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "63f66340",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 16\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30\n",
    "\n",
    "test_df = pd.concat([create_dataset(df, w) for w in test_weeks]).reset_index(drop=True)\n",
    "train_df = pd.concat([create_dataset(df, w) for w in train_weeks]).reset_index(drop=True)\n",
    "train_dataset = Dataset(train_df, SEQ_LEN)\n",
    "test_dataset = Dataset(test_df, SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model1 = NeuralAttentiveRecMachine(50)\n",
    "model2 = GruforRec(50)\n",
    "\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=LEARNING_RATE)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "b14aa7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Current Epoch 1\n",
      "------------------------------\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:36<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:14<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.33987345914627\n",
      "Test Loss: 0.32601783988645855\n",
      "Train Recall: 0.6316839456558228\n",
      "Test Recall: 0.638315737247467\n",
      "Train Precision: 0.0003657522611320019\n",
      "Test Precision: 0.0003516659198794514\n",
      "==============================\n",
      "Current Epoch 2\n",
      "------------------------------\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:37<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:14<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Train Loss: 0.31914361438686023\n",
      "Test Loss: 0.3134282427628182\n",
      "Train Recall: 0.6500526666641235\n",
      "Test Recall: 0.6564481258392334\n",
      "Train Precision: 0.00036366633139550686\n",
      "Test Precision: 0.00035167610622011125\n",
      "==============================\n",
      "Current Epoch 3\n",
      "------------------------------\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:38<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:14<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Train Loss: 0.31048621408757354\n",
      "Test Loss: 0.30778643117790555\n",
      "Train Recall: 0.6626895070075989\n",
      "Test Recall: 0.6657165288925171\n",
      "Train Precision: 0.00036280008498579264\n",
      "Test Precision: 0.0003509551752358675\n",
      "==============================\n",
      "Current Epoch 4\n",
      "------------------------------\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:37<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:14<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Train Loss: 0.30635916249542877\n",
      "Test Loss: 0.30485942778098424\n",
      "Train Recall: 0.6695542335510254\n",
      "Test Recall: 0.6733287572860718\n",
      "Train Precision: 0.00036190691753290594\n",
      "Test Precision: 0.0003511634422466159\n",
      "==============================\n",
      "Current Epoch 5\n",
      "------------------------------\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:38<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:15<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Train Loss: 0.3040814986484294\n",
      "Test Loss: 0.30308920541545564\n",
      "Train Recall: 0.6733088493347168\n",
      "Test Recall: 0.677336573600769\n",
      "Train Precision: 0.0003616934409365058\n",
      "Test Precision: 0.0003508933004923165\n",
      "==============================\n",
      "Current Epoch 6\n",
      "------------------------------\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:38<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:15<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "Train Loss: 0.30258610091913896\n",
      "Test Loss: 0.30185355865403324\n",
      "Train Recall: 0.6757940053939819\n",
      "Test Recall: 0.6797837615013123\n",
      "Train Precision: 0.00036127035855315626\n",
      "Test Precision: 0.00035071143065579236\n",
      "==============================\n",
      "Current Epoch 7\n",
      "------------------------------\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:42<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:16<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "Train Loss: 0.3015028140049582\n",
      "Test Loss: 0.30088414967538313\n",
      "Train Recall: 0.6783797144889832\n",
      "Test Recall: 0.6819550395011902\n",
      "Train Precision: 0.0003616877074819058\n",
      "Test Precision: 0.00035054254112765193\n",
      "==============================\n",
      "Current Epoch 8\n",
      "------------------------------\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████▌           | 221/258 [00:38<00:06,  5.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[481], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m train_precision \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m model1\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article_hist, target \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(model1(article_hist), target)\n\u001b[0;32m     13\u001b[0m     _loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\example_env\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\example_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\example_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\example_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\example_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[475], line 20\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     18\u001b[0m     article_hist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(row\u001b[38;5;241m.\u001b[39marticle_id[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len:])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     article_hist[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(row\u001b[38;5;241m.\u001b[39marticle_id):] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(row\u001b[38;5;241m.\u001b[39marticle_id)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m article_hist, target\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Current Epoch {i+1}\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Training..\")\n",
    "    \n",
    "    _loss = .0\n",
    "    train_recall = []\n",
    "    train_precision = []\n",
    "    model1.train()\n",
    "    for article_hist, target in tqdm(train_loader):\n",
    "        loss = criterion(model1(article_hist), target)\n",
    "        _loss += loss.item()\n",
    "        train_recall.append(eval_recall(model1(article_hist), target))\n",
    "        train_precision.append(eval_precision(model1(article_hist), target))\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "    \n",
    "    print(\"Evaluating..\")\n",
    "    _eval_loss = .0\n",
    "    test_recall = []\n",
    "    test_precision = []\n",
    "    model1.eval()\n",
    "    for article_hist, target in tqdm(test_loader):\n",
    "        eval_loss = criterion(model1(article_hist), target)\n",
    "        test_recall.append(eval_recall(model1(article_hist), target))\n",
    "        test_precision.append(eval_precision(model1(article_hist), target))\n",
    "\n",
    "        _eval_loss += eval_loss.item()\n",
    "    \n",
    "    train_loss = _loss/len(train_loader.dataset) * 100\n",
    "    eval_loss = _eval_loss/len(test_loader.dataset) * 100\n",
    "    train_recall = torch.mean(torch.tensor(train_recall))\n",
    "    test_recall = torch.mean(torch.tensor(test_recall))\n",
    "    train_precision = torch.mean(torch.tensor(train_precision))\n",
    "    test_precision = torch.mean(torch.tensor(test_precision))\n",
    "    print(f\"Epoch {i+1}\\nTrain Loss: {train_loss}\\nTest Loss: {eval_loss}\\nTrain Recall: {train_recall}\\nTest Recall: {test_recall}\\nTrain Precision: {train_precision}\\nTest Precision: {test_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Current Epoch {i+1}\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Training..\")\n",
    "    \n",
    "    _loss = .0\n",
    "    train_recall = []\n",
    "    train_precision = []\n",
    "    model2.train()\n",
    "    for article_hist, target in tqdm(train_loader):\n",
    "        loss = criterion(model2(article_hist), target)\n",
    "        _loss += loss.item()\n",
    "        train_recall.append(eval_recall(model2(article_hist), target))\n",
    "        train_precision.append(eval_precision(model2(article_hist), target))\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "    \n",
    "    print(\"Evaluating..\")\n",
    "    _eval_loss = .0\n",
    "    test_recall = []\n",
    "    test_precision = []\n",
    "    model2.eval()\n",
    "    for article_hist, target in tqdm(test_loader):\n",
    "        eval_loss = criterion(model2(article_hist), target)\n",
    "        test_recall.append(eval_recall(model2(article_hist), target))\n",
    "        test_precision.append(eval_precision(model2(article_hist), target))\n",
    "\n",
    "        _eval_loss += eval_loss.item()\n",
    "    \n",
    "    train_loss = _loss/len(train_loader.dataset) * 100\n",
    "    eval_loss = _eval_loss/len(test_loader.dataset) * 100\n",
    "    train_recall = torch.mean(torch.tensor(train_recall))\n",
    "    test_recall = torch.mean(torch.tensor(test_recall))\n",
    "    train_precision = torch.mean(torch.tensor(train_precision))\n",
    "    test_precision = torch.mean(torch.tensor(test_precision))\n",
    "    print(f\"Epoch {i+1}\\nTrain Loss: {train_loss}\\nTest Loss: {eval_loss}\\nTrain Recall: {train_recall}\\nTest Recall: {test_recall}\\nTrain Precision: {train_precision}\\nTest Precision: {test_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ba5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
